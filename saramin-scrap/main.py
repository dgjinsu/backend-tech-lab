import logging
import time
import random
from datetime import datetime
from pathlib import Path

from src.utils.browser import BrowserManager
from src.scraper.listing_scraper import ListingScraper
from src.scraper.company_scraper import CompanyScraper
from src.utils.pdf_export import export_to_pdf
from src.config import MIN_DELAY, MAX_DELAY, OUTPUT_DIR, LOG_LEVEL

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/saramin_scraper.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸš€ ì‚¬ëŒì¸ ì±„ìš© ê³µê³  í¬ë¡¤ëŸ¬")
    print("=" * 80)
    print()

    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    print("ğŸ“‹ í¬ë¡¤ë§í•  ì±„ìš© ê³µê³  ë¦¬ìŠ¤íŠ¸ URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    print("   ì˜ˆì‹œ: https://www.saramin.co.kr/zf_user/jobs/list/job-category?page=1&cat_kewd=291,238,235,292")
    url = input("URL: ").strip()
    print()

    print("ğŸ“„ í¬ë¡¤ë§í•  í˜ì´ì§€ ë²”ìœ„ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    while True:
        try:
            start_page = int(input("   ì‹œì‘ í˜ì´ì§€ (ì˜ˆ: 1): ").strip())
            if start_page < 1:
                print("   âš ï¸ 1 ì´ìƒì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            break
        except ValueError:
            print("   âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    while True:
        try:
            end_page = int(input("   ë í˜ì´ì§€ (ì˜ˆ: 5): ").strip())
            if end_page < start_page:
                print(f"   âš ï¸ ì‹œì‘ í˜ì´ì§€({start_page})ë³´ë‹¤ í° ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            break
        except ValueError:
            print("   âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    print()

    print("ğŸ“ ê²°ê³¼ íŒŒì¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (Enter í‚¤ë¥¼ ëˆ„ë¥´ë©´ ìë™ ìƒì„±).")
    output_name = input("   íŒŒì¼ëª… (ì˜ˆ: my_companies.pdf): ").strip()
    print()

    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    if output_name:
        # í™•ì¥ìê°€ ì—†ìœ¼ë©´ .pdf ì¶”ê°€
        if not output_name.endswith('.pdf'):
            output_name += '.pdf'
        output_path = Path(OUTPUT_DIR) / output_name
    else:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = Path(OUTPUT_DIR) / f'saramin_{timestamp}.pdf'

    logger.info("=" * 80)
    logger.info("ğŸš€ ì‚¬ëŒì¸ í¬ë¡¤ëŸ¬ ì‹œì‘")
    logger.info(f"ğŸ“Œ URL: {url}")
    logger.info(f"ğŸ“Œ í˜ì´ì§€ ë²”ìœ„: {start_page} ~ {end_page}")
    logger.info(f"ğŸ“Œ ì¶œë ¥ íŒŒì¼: {output_path}")
    logger.info("=" * 80)

    companies = []
    total_companies = 0

    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        with BrowserManager(headless=False) as driver:
            listing_scraper = ListingScraper(driver)
            company_scraper = CompanyScraper(driver)

            # ê° í˜ì´ì§€ í¬ë¡¤ë§
            for page in range(start_page, end_page + 1):
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ“„ [{page}/{end_page}] í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
                logger.info(f"{'='*60}")

                try:
                    # 1. ê³µê³  ë¦¬ìŠ¤íŠ¸ì—ì„œ íšŒì‚¬ URL ì¶”ì¶œ
                    company_urls = listing_scraper.scrape_listing_page(url, page)

                    if not company_urls:
                        logger.warning(f"âš ï¸ í˜ì´ì§€ {page}ì—ì„œ íšŒì‚¬ URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                        continue

                    # 2. ê° íšŒì‚¬ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
                    for idx, url in enumerate(company_urls, 1):
                        logger.info(f"  [{idx}/{len(company_urls)}] í¬ë¡¤ë§ ì¤‘...")

                        try:
                            company = company_scraper.scrape_company(url)
                            companies.append(company.to_dict())
                            total_companies += 1

                            # í¬ë¡¤ë§ ì†ë„ ì œì–´ (2-3ì´ˆ ëŒ€ê¸°)
                            delay = random.uniform(MIN_DELAY, MAX_DELAY)
                            time.sleep(delay)

                        except Exception as e:
                            logger.error(f"  âŒ íšŒì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")
                            continue

                    # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                    time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))

                except Exception as e:
                    logger.error(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue

            # PDF ì €ì¥
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
            export_to_pdf(companies, str(output_path))

            # ì™„ë£Œ ë©”ì‹œì§€
            logger.info("=" * 80)
            logger.info("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ì´ {total_companies}ê°œ íšŒì‚¬ ì •ë³´ ìˆ˜ì§‘")
            logger.info(f"ğŸ“ íŒŒì¼ ì €ì¥: {output_path}")
            logger.info("=" * 80)

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        if companies:
            logger.info("ğŸ’¾ ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥ ì¤‘...")
            export_to_pdf(companies, str(output_path))
            logger.info(f"ğŸ“ íŒŒì¼ ì €ì¥: {output_path}")

    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        if companies:
            logger.info("ğŸ’¾ ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥ ì¤‘...")
            export_to_pdf(companies, str(output_path))


if __name__ == '__main__':
    main()
