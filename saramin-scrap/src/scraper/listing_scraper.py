from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from typing import List
import logging
import time

logger = logging.getLogger(__name__)

class ListingScraper:
    """ì±„ìš© ê³µê³  ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìŠ¤í¬ë˜í¼"""

    def __init__(self, driver):
        self.driver = driver

    def scrape_listing_page(self, base_url: str, page: int) -> List[str]:
        """íŠ¹ì • í˜ì´ì§€ì˜ ê³µê³  ë¦¬ìŠ¤íŠ¸ì—ì„œ íšŒì‚¬ URL ì¶”ì¶œ

        Args:
            base_url: ê¸°ë³¸ URL (í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì œì™¸)
            page: í˜ì´ì§€ ë²ˆí˜¸

        Returns:
            íšŒì‚¬ ìƒì„¸ í˜ì´ì§€ URL ë¦¬ìŠ¤íŠ¸
        """
        # URL ìƒì„± (page íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸)
        url = self._build_url(base_url, page)
        logger.info(f"ğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹œì‘: {url}")

        try:
            # í˜ì´ì§€ ë¡œë“œ
            self.driver.get(url)

            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ë” ê¸´ ì‹œê°„)
            time.sleep(3)

            # í˜ì´ì§€ ì†ŒìŠ¤ íŒŒì‹±
            soup = BeautifulSoup(self.driver.page_source, 'lxml')

            # ë””ë²„ê·¸: HTML ì¼ë¶€ ì €ì¥
            logger.debug(f"í˜ì´ì§€ HTML ê¸¸ì´: {len(soup.text)}")

            # íšŒì‚¬ URL ì¶”ì¶œ
            company_urls = self._extract_company_urls(soup)

            logger.info(f"âœ… í˜ì´ì§€ {page}: {len(company_urls)}ê°œ íšŒì‚¬ URL ì¶”ì¶œ")
            return company_urls

        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []

    def _build_url(self, base_url: str, page: int) -> str:
        """í˜ì´ì§€ ë²ˆí˜¸ê°€ í¬í•¨ëœ URL ìƒì„±"""
        if 'page=' in base_url:
            # ê¸°ì¡´ page íŒŒë¼ë¯¸í„° êµì²´
            import re
            return re.sub(r'page=\d+', f'page={page}', base_url)
        else:
            # page íŒŒë¼ë¯¸í„° ì¶”ê°€
            separator = '&' if '?' in base_url else '?'
            return f"{base_url}{separator}page={page}"

    def _extract_company_urls(self, soup: BeautifulSoup) -> List[str]:
        """ê³µê³  ë¦¬ìŠ¤íŠ¸ì—ì„œ íšŒì‚¬ ë‚´ë¶€ ì±„ìš© URL ì¶”ì¶œ

        ì‹¤ì œ íë¦„:
        1. ê³µê³  ë¦¬ìŠ¤íŠ¸ â†’ view-inner-recruit URL ì¶”ì¶œ
        2. ë‚˜ì¤‘ì— CompanyScraperì—ì„œ view-inner-recruit â†’ view?csnìœ¼ë¡œ ë³€í™˜
        """
        urls = []
        from src.config import SARAMIN_BASE_URL

        # a.str_tit ë§í¬ ì°¾ê¸° (íšŒì‚¬ ì´ë¦„ ë§í¬)
        company_links = soup.select('a.str_tit[href*="company-info/view-inner-recruit"]')

        logger.info(f"âœ… {len(company_links)}ê°œ íšŒì‚¬ ë§í¬ ë°œê²¬")

        for link in company_links:
            href = link.get('href')
            company_name = link.get_text(strip=True)

            if href:
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                if href.startswith('/'):
                    href = SARAMIN_BASE_URL + href

                urls.append(href)
                logger.debug(f"  - {company_name}: {href}")

        # ì¤‘ë³µ ì œê±°
        return list(set(urls))
